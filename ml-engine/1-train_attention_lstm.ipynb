{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b918f061",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Attention-LSTM Training Notebook\n",
    "\n",
    "This notebook loads historical price data from all asset classes, prepares time-series sequences, trains an Attention-based LSTM model, and saves it as `trained-attention-lstm.pth`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68385f74",
   "metadata": {},
   "source": [
    "## [Block 1] Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bfb96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e782f",
   "metadata": {},
   "source": [
    "## [Block 2] Attention-Based LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b7f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=64, output_dim=1):\n",
    "        super(AttentionLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def attention(self, lstm_output):\n",
    "        weights = torch.softmax(self.attn(lstm_output).squeeze(-1), dim=1)\n",
    "        context = torch.bmm(weights.unsqueeze(1), lstm_output).squeeze(1)\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        context = self.attention(lstm_out)\n",
    "        return self.fc(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22502387",
   "metadata": {},
   "source": [
    "## [Block 3] Load Historical Data from All Asset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d672cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_time_series(data_root='data/historical-data', seq_len=60):\n",
    "    X, y = [], []\n",
    "    for root, _, files in os.walk(data_root):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                df = pd.read_csv(os.path.join(root, file))\n",
    "                if 'Close' not in df.columns or len(df) < seq_len + 1:\n",
    "                    continue\n",
    "                df['log_return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "                df.dropna(inplace=True)\n",
    "                series = df['log_return'].values[-(seq_len + 1):]\n",
    "                scaled = MinMaxScaler().fit_transform(series.reshape(-1, 1)).flatten()\n",
    "                X.append(scaled[:-1])\n",
    "                y.append(scaled[-1])\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)\n",
    "    return X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27002e07",
   "metadata": {},
   "source": [
    "## [Block 4] Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52bb49a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/finsight/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/miniconda3/envs/finsight/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/miniconda3/envs/finsight/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/miniconda3/envs/finsight/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/miniconda3/envs/finsight/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/miniconda3/envs/finsight/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/miniconda3/envs/finsight/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/miniconda3/envs/finsight/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/miniconda3/envs/finsight/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/miniconda3/envs/finsight/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/miniconda3/envs/finsight/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 544\n",
      "Epoch 00 - Loss: 3.751016 | Train Acc: 0.00% | Val Acc: 0.00%\n",
      "Epoch 10 - Loss: 0.513614 | Train Acc: 22.53% | Val Acc: 27.52%\n",
      "Epoch 20 - Loss: 0.486272 | Train Acc: 24.60% | Val Acc: 27.52%\n",
      "Epoch 30 - Loss: 0.453530 | Train Acc: 29.43% | Val Acc: 33.94%\n",
      "Epoch 40 - Loss: 0.428097 | Train Acc: 34.71% | Val Acc: 33.03%\n",
      "Epoch 50 - Loss: 0.432132 | Train Acc: 35.86% | Val Acc: 38.53%\n",
      "Epoch 60 - Loss: 0.412836 | Train Acc: 34.71% | Val Acc: 27.52%\n",
      "Epoch 70 - Loss: 0.430031 | Train Acc: 32.64% | Val Acc: 33.03%\n",
      "Epoch 80 - Loss: 0.426731 | Train Acc: 35.17% | Val Acc: 44.04%\n",
      "Epoch 90 - Loss: 0.406952 | Train Acc: 37.47% | Val Acc: 38.53%\n",
      "\n",
      "-> Final Validation Accuracy: 46.79%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Accuracy metric: % of predictions within Â±10% of actual value\n",
    "def percentage_accuracy(y_true, y_pred, tolerance=0.10):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    within_tolerance = np.abs(y_true - y_pred) <= (tolerance * np.abs(y_true))\n",
    "    return np.mean(within_tolerance) * 100\n",
    "\n",
    "# Load and process time series from all assets\n",
    "def load_all_time_series(data_root, seq_len=60):\n",
    "    X, y = [], []\n",
    "    for root, _, files in os.walk(data_root):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                df = pd.read_csv(os.path.join(root, file))\n",
    "\n",
    "                if 'Close' not in df.columns or len(df) < seq_len + 2:\n",
    "                    continue\n",
    "\n",
    "                # Ensure 'Close' is numeric\n",
    "                df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "\n",
    "                # ðŸ” Calculate log returns safely\n",
    "                df['log_return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "                df['log_return'] = df['log_return'].replace([np.inf, -np.inf], np.nan)\n",
    "                df.dropna(subset=['log_return'], inplace=True)\n",
    "\n",
    "                series = df['log_return'].values[-(seq_len + 1):]\n",
    "                if len(series) < seq_len + 1:\n",
    "                    continue\n",
    "\n",
    "                scaled = MinMaxScaler().fit_transform(series.reshape(-1, 1)).flatten()\n",
    "                X.append(scaled[:-1])\n",
    "                y.append(scaled[-1])\n",
    "    \n",
    "    # Use np.array() to avoid torch tensor warnings\n",
    "    X_tensor = torch.tensor(np.array(X), dtype=torch.float32).unsqueeze(-1)\n",
    "    y_tensor = torch.tensor(np.array(y), dtype=torch.float32).unsqueeze(-1)\n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "X, y = load_all_time_series(data_root='./data/historical-data')\n",
    "print(\"Training samples:\", X.shape[0])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32)\n",
    "\n",
    "# âš™ï¸ Model, loss, optimizer\n",
    "model = AttentionLSTM()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_train_preds = []\n",
    "    all_train_targets = []\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X)\n",
    "        loss = loss_fn(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        all_train_preds.extend(output.detach().cpu().numpy())\n",
    "        all_train_targets.extend(batch_y.detach().cpu().numpy())\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            output = model(batch_X)\n",
    "            val_preds.extend(output.cpu().numpy())\n",
    "            val_targets.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    train_acc = percentage_accuracy(all_train_targets, all_train_preds)\n",
    "    val_acc = percentage_accuracy(val_targets, val_preds)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:02d} - Loss: {total_loss:.6f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "\n",
    "# Final result\n",
    "print(f\"\\n-> Final Validation Accuracy: {best_val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eb2c9b",
   "metadata": {},
   "source": [
    "## [Block 5] Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27e5356e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model saved as trained-attention-lstm.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'trained-attention-lstm.pth')\n",
    "print(\" Model saved as trained-attention-lstm.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finsight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
